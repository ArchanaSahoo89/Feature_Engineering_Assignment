{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArchanaSahoo89/Feature_Engineering_Assignment/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. What is a parameter?\n",
        "\n",
        "# Ans:\n",
        "\n",
        "# In feature engineering, a parameter typically refers to any variable or configuration that influences the transformation, creation, or selection of features from raw data. Feature engineering is the process of preparing and transforming data to improve the performance of machine learning models, and parameters play a key role in shaping this process.\n",
        "\n",
        "# Here are a few ways parameters are used in feature engineering:\n",
        "\n",
        "# 1. Transformation Parameters\n",
        "# When applying transformations to features, certain parameters control the transformation process. For example:\n",
        "\n",
        "# Scaling and Normalization: Parameters like the mean and standard deviation for z-score normalization, or a scaling factor for min-max scaling.\n",
        "# Binning: Parameters such as the number of bins or the bin edges used when discretizing continuous features into categorical bins.\n",
        "# Encoding: Parameters may control how categorical variables are encoded, like the number of top categories kept in one-hot encoding or the frequency threshold in target encoding.\n",
        "\n",
        "# 2. Feature Extraction Parameters\n",
        "\n",
        "# When creating new features, the parameters govern the feature extraction process. For instance:\n",
        "\n",
        "# In text processing, parameters like n-gram range (e.g., unigrams, bigrams, trigrams) are used to determine the set of features created from the text.\n",
        "# For time-series data, parameters like window size or lag might define how historical data is used to create new features (e.g., rolling averages).\n",
        "\n",
        "# 3. Feature Selection Parameters\n",
        "\n",
        "# During feature selection, parameters help determine which features to keep based on their relevance to the target variable. Examples include:\n",
        "\n",
        "# Thresholds for correlation: A parameter might define the minimum correlation value above which features are considered to be highly correlated and should be removed.\n",
        "# Selection algorithms: Parameters might define the criteria for selecting features, such as the number of top features to select (e.g., using mutual information or L1 regularization).\n",
        "\n",
        "# 4. Hyperparameters in Feature Engineering Algorithms\n",
        "\n",
        "# Some feature engineering methods, like dimensionality reduction or clustering, use hyperparameters that control the algorithm's behavior:\n",
        "\n",
        "# In Principal Component Analysis (PCA), the number of principal components to retain is a parameter.\n",
        "# In Clustering algorithms (e.g., k-means), the number of clusters (k) is a key parameter.\n",
        "\n",
        "# 5. Domain-Specific Parameters\n",
        "\n",
        "# In some cases, domain knowledge is used to define parameters that influence feature creation:\n",
        "\n",
        "# For instance, in a financial dataset, time intervals or transaction thresholds might be used as parameters\n",
        "# to create aggregated features like monthly or quarterly sums or averages.\n",
        "\n"
      ],
      "metadata": {
        "id": "INa2wafUaxVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What is correlation?\n",
        "# What does negative correlation mean?\n",
        "\n",
        "#Ans:\n",
        "\n",
        "# Correlation refers to a statistical relationship between two or more variables, indicating how changes in one variable might be associated with changes in another. It helps to measure the strength and direction of the linear relationship between variables. In simpler terms, correlation tells us whether, and how strongly, two variables are connected or related to each other.\n",
        "\n",
        "# Correlation is usually measured using a statistic called the 'correlation coefficient', which ranges from -1 to +1:\n",
        "\n",
        "# +1: Perfect positive correlation — as one variable increases, the other increases in a perfectly linear manner.\n",
        "# 0: No correlation — there is no linear relationship between the two variables.\n",
        "# -1: Perfect negative correlation — as one variable increases, the other decreases in a perfectly linear manner.\n",
        "\n",
        "# 'Negative correlation' occurs when two variables move in opposite directions.\n",
        "# Specifically, when one variable increases, the other decreases, and vice versa.\n",
        "# In this case, the correlation coefficient is between 0 and -1. The closer the correlation coefficient is to -1, the stronger the negative correlation.\n",
        "\n",
        "# For example:\n",
        "\n",
        "# Example 1: Temperature and heating bills – As the temperature increases, heating bills typically decrease. This would represent a **negative correlation**, where the two variables move in opposite directions.\n",
        "# Example 2: Study time and test errors – If more time spent studying is associated with fewer errors on a test, that would indicate a negative correlation (fewer errors when study time increases).\n",
        "\n"
      ],
      "metadata": {
        "id": "BJ7TDYQYfMNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "#Ans:\n",
        "\n",
        "# Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on creating algorithms and models\n",
        "# that allow computers to learn from data and make predictions or decisions without being explicitly programmed.\n",
        "# Instead of following predefined rules, a machine learning system improves its performance over time by recognizing patterns in the data.\n",
        "\n",
        "# In essence, machine learning enables systems to learn from experience and adapt based on new data,\n",
        "# making them more accurate and efficient as they process more information.\n",
        "\n",
        "# Main Components in Machine Learning:\n",
        "\n",
        "# Machine learning involves several key components that work together to build a successful model. These components include:\n",
        "\n",
        "# Data\n",
        "\n",
        "# Data is the foundation of machine learning. It is used to train the model, allowing it to learn patterns and make predictions.\n",
        "# Data can be structured (e.g., spreadsheets, databases) or unstructured (e.g., images, text, audio).\n",
        "# The data is typically split into training data (used to train the model), validation data (used to tune hyperparameters)\n",
        "# and test data (used to evaluate the final model's performance).\n",
        "\n",
        "# Algorithms\n",
        "\n",
        "# Machine learning algorithms are the mathematical and statistical methods that help identify patterns in data.\n",
        "# These algorithms learn from data and make predictions or decisions. There are several types of ML algorithms:\n",
        "# Supervised learning (e.g., linear regression, decision trees, neural networks) — algorithms learn from labeled data (data with known outcomes).\n",
        "# Unsupervised learning (e.g., clustering, PCA) — algorithms learn from unlabeled data and try to find patterns or groupings.\n",
        "# Reinforcement learning — algorithms learn by interacting with an environment and receiving feedback (rewards or penalties).\n",
        "# Semi-supervised and Self-supervised learning — a hybrid approach using both labeled and unlabeled data.\n",
        "\n",
        "#Model\n",
        "\n",
        "# A model is the mathematical representation of the learned patterns from data. It's created by training an algorithm on the training dataset. Once trained, the model can make predictions or decisions based on new data.\n",
        "# The model evolves through learning, adjusting its internal parameters to minimize errors in its predictions or classifications.\n",
        "\n",
        "# Features (or Variables)\n",
        "\n",
        "# Features are the individual measurable properties or characteristics of the data used by the model to learn patterns. Features represent the input to a machine learning algorithm and can be numeric, categorical, or textual.\n",
        "# Feature engineering is the process of selecting, modifying, or creating new features to improve model performance.\n",
        "\n",
        "# Training\n",
        "\n",
        "# Training is the process of using data to teach a machine learning model how to make predictions or decisions. During training, the model's parameters (such as weights in a neural network) are adjusted to minimize errors based on a defined loss function.\n",
        "# The training process involves the optimization of the model, often using techniques like gradient descent.\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "# Evaluation is the process of testing how well the trained model performs on new, unseen data (often using a test dataset).\n",
        "# Evaluation metrics depend on the task but can include accuracy, precision, recall, F1 score, mean squared error (MSE), etc.\n",
        "# Evaluation helps determine how well the model generalizes to real-world scenarios.\n",
        "\n",
        "# Hyperparameters\n",
        "\n",
        "# Hyperparameters are the settings or configurations external to the model that influence the training process. These parameters need to be set before training and include things like the learning rate, the number of layers in a neural network, or the number of trees in a random forest. Hyperparameter tuning is the process of optimizing these parameters to improve model performance.\n",
        "\n",
        "# Prediction (or Inference)\n",
        "\n",
        "#  Prediction is the final step where the trained model is used to make decisions or forecast future data. When the model is given new data, it applies the patterns and relationships learned during training to make predictions.\n",
        "\n",
        "# Loss Function\n",
        "\n",
        "# The loss function (also called cost function) is a key component in machine learning that quantifies how well the model is performing. It calculates the difference between the model's prediction and the actual result. The goal during training is to minimize this loss, improving the model’s accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ku50d8C7fL2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "\n",
        "# Ans:\n",
        "\n",
        "# The loss value is a key indicator in machine learning and deep learning for determining how well a model is performing.\n",
        "# It quantifies the difference between the model's predictions and the actual values (ground truth) in the dataset.\n",
        "# Here's how the loss value helps in evaluating the model:\n",
        "\n",
        "# 1. Indicates Model Accuracy\n",
        "\n",
        "# A lower loss value typically indicates that the model's predictions are closer to the actual values, meaning the model is performing well.\n",
        "# A higher loss value suggests that the model's predictions are far from the actual values, meaning the model needs improvement.\n",
        "\n",
        "# 2. Guides the Optimization Process\n",
        "\n",
        "# During training, the model updates its parameters (weights) to minimize the loss function. This is usually done using an optimization algorithm like Gradient Descent. By monitoring the loss value, you can assess how well the model is learning:\n",
        "# If the loss is decreasing over time, the model is improving and learning from the data.\n",
        "# If the loss is not changing or increasing, the model is not learning well and might need adjustments in terms of architecture, data, or hyperparameters.\n",
        "\n",
        "# 3. Helps in Hyperparameter Tuning\n",
        "\n",
        "# The loss value helps in tuning the model's hyperparameters (e.g., learning rate, batch size, etc.). A loss function is often used to find the best combination of hyperparameters that minimize the loss, which in turn improves the model's performance.\n",
        "\n",
        "# 4. Comparing Different Models or Configurations\n",
        "\n",
        "# The loss value can be used to compare different models, architectures, or configurations. A model with a lower loss on a validation set is generally considered better, as it implies the model generalizes well to unseen data.\n",
        "# However, it's important to not rely solely on the loss value but also to consider other metrics like accuracy, precision, recall, and F1 score to get a full picture of model performance.\n",
        "\n",
        "# 5. Helps to Detect Overfitting or Underfitting\n",
        "\n",
        "# Overfitting occurs when the model performs well on the training set but poorly on the validation/test set. This can be seen if the training loss keeps decreasing, but the validation loss starts to increase. In such cases, the model has learned to memorize the training data but is not able to generalize to new data.\n",
        "# Underfitting happens when the model is too simple or lacks the capacity to learn the underlying patterns. This is typically reflected in both high training and validation loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "4CMoYADGfKpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "# Ans:\n",
        "\n",
        "# Continuous variables and categorical variables are two types of variables that are commonly used in data analysis and machine learning.\n",
        "# They differ based on the nature of the data they represent and how they are treated in statistical modeling. Here's an explanation of both:\n",
        "\n",
        "# 1. Continuous Variables\n",
        "\n",
        "# Continuous variables, also known as quantitative variables, represent measurable quantities that can take any value within a given range. These values are numerical and can be infinitely divided into smaller increments.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Can take on an infinite number of values.\n",
        "# Often represent measurements such as height, weight, temperature, age, or distance.\n",
        "# Typically represented by floating-point numbers (e.g., 5.7, 10.2, 30.45).\n",
        "# Can be subjected to arithmetic operations (addition, subtraction, multiplication, etc.).\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Height: 175.3 cm, 180.1 cm.\n",
        "# Weight: 70.5 kg, 68.3 kg.\n",
        "# Temperature: 21.4°C, 37.6°C.\n",
        "# Time: 3.5 hours, 12.75 minutes.\n",
        "\n",
        "# Use in Machine Learning:\n",
        "# Continuous variables are often used as inputs (features) in regression models, where the goal is to predict a continuous outcome, or they can be used in clustering or classification tasks if the relationship between variables and outcomes is continuous.\n",
        "\n",
        "# 2. Categorical Variables\n",
        "\n",
        "# Categorical variables, also known as qualitative variables, represent categories or groups that can be used to label or classify data. These variables do not have a meaningful numerical order or scale but are often encoded as strings or numbers for ease of analysis.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Represent categories or groups.\n",
        "# Can take on a limited number of distinct values or labels.\n",
        "# The values of categorical variables cannot be meaningfully ordered or measured mathematically.\n",
        "# Can be either nominal or ordinal:\n",
        "\n",
        "# Nominal: Categories that do not have an inherent order (e.g., colors, types of animals, city names).\n",
        "\n",
        "# Ordinal: Categories that have a meaningful order or ranking, but the intervals between categories are not defined (e.g., ratings like \"poor,\" \"fair,\" \"good,\" \"excellent\").\n",
        "\n",
        "# Examples:\n",
        "\n",
        "# Gender: Male, Female, Non-binary.\n",
        "# City: New York, London, Tokyo.\n",
        "# Education Level (ordinal): High School, Bachelor's, Master's, Ph.D.\n",
        "# Survey Rating (ordinal): Poor, Fair, Good, Excellent.\n",
        "\n",
        "# Use in Machine Learning:\n",
        "# Categorical variables often need to be converted into a numerical format before they can be used in machine learning models (e.g., using one-hot encoding or label encoding).\n",
        "# They are commonly used in classification tasks, where the goal is to assign an observation to a specific category or class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "No6J6h5rfJ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "# Handling categorical variables in machine learning is an essential preprocessing step because most machine learning algorithms require numerical input. There are several techniques to transform categorical data into a format suitable for machine learning models. The common techniques include:\n",
        "\n",
        "# 1. Label Encoding\n",
        "\n",
        "# Label Encoding converts each category into a unique integer label. For example, a categorical variable like \"Color\" with values [\"Red\", \"Green\", \"Blue\"] could be encoded as [0, 1, 2].\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# Each unique category is assigned a numerical value, usually starting from 0.\n",
        "# For the above example, \"Red\" → 0, \"Green\" → 1, \"Blue\" → 2.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# Simple and fast.\n",
        "# Works well when the categorical variable has a natural ordering (ordinal).\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# If the categorical variable is nominal (no natural order), label encoding introduces unintended ordinal relationships between the categories, which can mislead some models (e.g., Linear Regression, Decision Trees).\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# When the categorical variable has a meaningful order (e.g., \"Low\", \"Medium\", \"High\").\n",
        "\n",
        "# 2. One-Hot Encoding\n",
        "\n",
        "# One-Hot Encoding is a technique that creates a new binary (0 or 1) column for each possible category in the original categorical variable. Each instance is represented by a 1 in the column corresponding to its category and 0 in all other columns.\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# For a categorical feature with N categories, you create N new columns.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# No assumptions about the data (no implied ordinal relationship).\n",
        "# Works well for nominal categorical variables (without any natural order).\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# Can increase the dimensionality of the dataset significantly if the categorical variable has many unique values (known as the \"curse of dimensionality\").\n",
        "# Leads to sparse matrices, which can be inefficient for some algorithms.\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# When categorical variables are nominal (no inherent order), such as Country, City, or Product Category.\n",
        "\n",
        "\n",
        "# 3. Ordinal Encoding (or Ordinal Integer Encoding)\n",
        "\n",
        "# Ordinal Encoding is used when the categorical variable has a natural order (ordinal data). Each category is mapped to an integer value that reflects its order or rank.\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# Categories are assigned integer values based on their order.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# Simple and does not create additional columns.\n",
        "# Suitable for ordinal variables, where the relationship between categories matters.\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# If the categories are not ordered in a meaningful way, this can introduce misleading relationships.\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# For features with inherent order, like Education Level, Rating scales (\"Low\", \"Medium\", \"High\"), etc.\n",
        "\n",
        "# 4. Frequency or Count Encoding\n",
        "\n",
        "# Frequency or Count Encoding replaces each category with its frequency (the number of occurrences of that category) or the count of its instances in the dataset.\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# For each category, count the number of occurrences.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# Retains information about the frequency of each category.\n",
        "\n",
        "# Can be useful for models like Decision Trees, which can use this as a feature to make splits.\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# Does not work well if frequency is similar across categories, as it might not introduce significant information.\n",
        "# Can still lead to issues with high cardinality.\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# When the frequency of the category might carry useful information, especially in decision-tree-based models.\n",
        "\n",
        "\n",
        "# 5. Target Encoding (Mean Encoding)\n",
        "\n",
        "# Target Encoding encodes the categories by replacing each category with the mean of the target variable for that category. This can be useful for supervised learning tasks where the feature's relationship with the target is important.\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# For a categorical variable, calculate the mean of the target variable for each category.\n",
        "# For example, if a feature \"Color\" is related to a target variable \"Price\", you would replace each color with the average price associated with that color.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# Can work well with high-cardinality categorical variables.\n",
        "# Captures the relationship between the categorical feature and the target variable.\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# Risk of data leakage if the target mean is computed from the whole dataset (including the test set).\n",
        "# Requires proper cross-validation to avoid overfitting.\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# Works well for regression tasks or binary classification tasks where the categorical feature has a relationship with the target variable.\n",
        "\n",
        "\n",
        "# 6. Hashing (Feature Hashing)\n",
        "\n",
        "# Hashing (or Feature Hashing) is a technique that reduces the dimensionality of categorical features by applying a hash function to the categories, converting them into a fixed-size numerical vector.\n",
        "\n",
        "# How it works:\n",
        "\n",
        "# A hash function is applied to the categories, and the result is used to map each category to a fixed number of bins (dimensions).\n",
        "# For instance, if you have a category \"City\" with many unique values (e.g., \"New York\", \"London\", \"Tokyo\"), hashing will assign each to a smaller number of bins based on a hash function.\n",
        "\n",
        "# Pros:\n",
        "\n",
        "# Handles high-cardinality categorical variables efficiently.\n",
        "# Reduces dimensionality and is computationally efficient.\n",
        "\n",
        "# Cons:\n",
        "\n",
        "# Can lead to hash collisions, where two different categories are mapped to the same hash value.\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "# Useful when dealing with categorical variables that have a large number of categories (e.g., e-commerce product IDs).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BbguuscjBz_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. What do you mean by training and testing a dataset?\n",
        "\n",
        "\n",
        "# Training and testing a dataset are key concepts in machine learning, used to evaluate the performance of a model.\n",
        "\n",
        "# Training a Dataset:\n",
        "\n",
        "# Training a dataset refers to the process of feeding data to a machine learning model so it can learn patterns and relationships within that data.\n",
        "# This is typically done by:\n",
        "\n",
        "# Inputting Features:\n",
        "# The dataset is divided into inputs (features) and outputs (labels). The features are used by the model to make predictions.\n",
        "\n",
        "# Model Learning:\n",
        "# The model adjusts its internal parameters to minimize the error between its predictions and the actual outputs (labels). This process is often done using optimization algorithms like gradient descent.\n",
        "# The training process allows the model to \"learn\" from the data, meaning it gets better at making predictions or classifications based on the examples it sees.\n",
        "\n",
        "# Testing a Dataset:\n",
        "\n",
        "# Testing a dataset is the process of evaluating how well the model performs on new, unseen data. After training the model, you use a separate subset of the data, called the test set, to measure its accuracy or performance. This helps to assess whether the model has:\n",
        "\n",
        "# Generalized well:\n",
        "# The model can make accurate predictions on data it hasn't seen before, indicating it is not just memorizing the training data (overfitting).\n",
        "\n",
        "# Performance Metrics:\n",
        "# Common metrics for evaluation include accuracy, precision, recall, F1-score, mean squared error (for regression), etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a0sdAhYDKweC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. What is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "\n",
        "# sklearn.preprocessing is a module in the scikit-learn library, a popular machine learning library in Python. It provides various tools and techniques to preprocess data before using it to train machine learning models.\n",
        "\n",
        "# Data preprocessing is an essential step in building machine learning models, as raw data often needs to be transformed into a suitable format for algorithms to perform effectively. The functions in sklearn.preprocessing are designed to make these transformations easy and efficient.\n",
        "\n",
        "# Here are some of the main tasks that sklearn.preprocessing helps with:\n",
        "\n",
        "# 1. Scaling/Normalizing Data\n",
        "\n",
        "#    Standardization (Z-score scaling): It transforms features so that they have a mean of 0 and a standard deviation of 1.\n",
        "#     StandardScaler: Scales features by removing the mean and scaling to unit variance.\n",
        "\n",
        "#    Min-Max Scaling (Normalization): It scales features to a specified range, usually [0, 1].\n",
        "#     MinMaxScaler: Scales features to a given range (default [0, 1]).\n",
        "\n",
        "#    MaxAbs Scaling: Scales each feature by its maximum absolute value.\n",
        "#     MaxAbsScaler: Scales the data by its maximum absolute value, preserving sparsity.\n",
        "\n",
        "\n",
        "# 2. Encoding Categorical Data\n",
        "\n",
        "#    Label Encoding: Converts categorical values into numerical values by assigning each unique category a number.\n",
        "#     LabelEncoder: Encodes categorical labels into integers.\n",
        "\n",
        "#    One-Hot Encoding: Converts categorical variables into binary (0 or 1) columns, where each category gets a separate column.\n",
        "#      OneHotEncoder: Creates a binary matrix for categorical data.\n",
        "\n",
        "#   Ordinal Encoding: Encodes ordinal data (data that has an inherent order, like ratings) with integer values.\n",
        "#    OrdinalEncoder: Encodes categorical features with integer values based on their order.\n",
        "\n",
        "# 3. Imputation (Handling Missing Data)\n",
        "\n",
        "#   Simple Imputation: Fills missing values with mean, median, or most frequent value.\n",
        "#    SimpleImputer: Handles missing data by replacing them with specified strategies (e.g., mean, median, most frequent).\n",
        "\n",
        "#   KNN Imputation: Uses nearest neighbors to impute missing values.\n",
        "#    KNNImputer: Uses K-nearest neighbors for imputation of missing values.\n",
        "\n",
        "# 4. Binarization\n",
        "#   Binarize: Converts numeric values into binary values (0 or 1) based on a threshold.\n",
        "\n",
        "#   Binarizer: Thresholds the values of data to either 0 or 1.\n",
        "\n",
        "# 5. Polynomial Features\n",
        "\n",
        "#   Polynomial Expansion: Generates new features by combining existing features using polynomial relationships.\n",
        "#    PolynomialFeatures: Generates polynomial and interaction features.\n",
        "\n",
        "# 6. Power Transformer\n",
        "\n",
        "#  Power Transformations: Used to make data more Gaussian-like, which can improve the performance of many machine learning algorithms.\n",
        "#   PowerTransformer: Applies a power transformation to make data more normal (Gaussian).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zhR14jkMM-8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. What is a Test set?\n",
        "\n",
        "\n",
        "\n",
        "# A test set is a subset of data that is used to evaluate the performance of a machine learning model after it has been trained. The primary purpose of the test set is to assess how well the model generalizes to new, unseen data. This evaluation helps ensure that the model is not just memorizing the training data (a problem known as overfitting) but is capable of making accurate predictions on new data.\n",
        "\n",
        "# Key Characteristics of a Test Set:\n",
        "# Unseen Data: The test set consists of data that the model has never encountered during training. It is completely separate from the training data and validation data (if any). This helps provide an unbiased estimate of the model's performance.\n",
        "\n",
        "# Model Evaluation: After a model has been trained on the training set, the test set is used to evaluate how well the model performs. Common performance metrics for evaluation include:\n",
        "\n",
        "# Accuracy\n",
        "# Precision\n",
        "# Recall\n",
        "# F1 Score\n",
        "# Mean Squared Error (MSE), etc., depending on the type of problem (classification, regression, etc.).\n",
        "\n",
        "# Prevents Overfitting: Using a separate test set helps to detect overfitting, which occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the performance on new data. If a model performs well on the training set but poorly on the test set, it might be overfitting.\n",
        "\n",
        "# Data Splitting:\n",
        "# In a typical machine learning workflow, the data is split into multiple subsets:\n",
        "\n",
        "# Training Set: The data used to train the model.\n",
        "# Validation Set (optional): A subset used during training to tune hyperparameters (not always used, depending on the workflow).\n",
        "# Test Set: The data used to assess the model's performance after training.\n",
        "\n",
        "# Example Workflow:\n",
        "# Data Splitting: The original dataset is split into a training set (usually around 70-80% of the data) and a test set (typically 20-30% of the data).\n",
        "# Model Training: The model is trained using the training data.\n",
        "# Model Testing: After training, the model is tested on the test set to evaluate how well it generalizes to unseen data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dHj8w8jz4wA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# When working with machine learning in Python, we typically split the available data into two primary subsets:\n",
        "\n",
        "# Training Data: Used to train the model and learn patterns from the data.\n",
        "# Testing Data: Used to evaluate the model's performance on unseen data (i.e., to test how well the model generalizes).\n",
        "# A common approach is to split the dataset into 80% training and 20% testing, though this can vary based on the dataset size and problem type. You can use train_test_split from sklearn.model_selection to perform this task.\n",
        "\n",
        "# X: Features (independent variables)\n",
        "# y: Target (dependent variable)\n",
        "# test_size=0.2: Specifies 20% of the data will be used for testing.\n",
        "# random_state=42: Ensures reproducibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "bBfAgeXR7PAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. How do you approach a Machine Learning problem?\n",
        "\n",
        "# The process of approaching a machine learning problem can be broken down into several steps. Here's a general workflow:\n",
        "\n",
        "# 1. Problem Definition\n",
        "\n",
        "# Understand the problem to be solved. Is it a classification, regression or another type of machine learning task.\n",
        "# What is the objective, and what are the inputs and expected outputs.\n",
        "\n",
        "# 2. Data Collection\n",
        "\n",
        "# Gather the dataset or use an existing one. Make sure it contains relevant features that may help in solving the problem.\n",
        "# If the dataset is too small, consider augmenting it.\n",
        "\n",
        "# 3. Data Preprocessing\n",
        "\n",
        "# Cleaning: Remove missing values or impute them. Handle outliers, duplicates, etc.\n",
        "# Feature Engineering: Create new features from existing ones, extract meaningful information.\n",
        "# Normalization/Standardization: Scale numerical features to a standard range (e.g., [0, 1]) or standard deviation.\n",
        "# Categorical Encoding: Convert categorical variables (e.g., gender, city) into numerical representations (e.g., one-hot encoding, label encoding).\n",
        "# Data Splitting: Split the data into training and test sets (as shown earlier).\n",
        "\n",
        "# 4. Model Selection\n",
        "\n",
        "# Choose a Model: Select an appropriate algorithm based on the problem type (e.g., Linear Regression for regression, Decision Trees, Random Forests, or SVM for classification).\n",
        "# Baseline Model: Start with a simple model to create a baseline performance.\n",
        "\n",
        "# 5. Model Training\n",
        "\n",
        "# Train the selected model on the training dataset.\n",
        "# Fit the model and learn the relationships in the data.\n",
        "\n",
        "# 6. Model Evaluation\n",
        "\n",
        "# Evaluate the model on the test dataset. Common metrics include:\n",
        "# For classification: Accuracy, Precision, Recall, F1-score, Confusion Matrix.\n",
        "# For regression: Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared.\n",
        "# Use cross-validation if needed to evaluate the model on different subsets of the data (helps assess generalization).\n",
        "\n",
        "# 7. Hyperparameter Tuning\n",
        "\n",
        "# Use techniques like GridSearchCV or RandomizedSearchCV to tune hyperparameters for better performance.\n",
        "\n",
        "# 8. Model Evaluation & Validation\n",
        "\n",
        "# After tuning hyperparameters, retrain the model on the entire training dataset and evaluate it again using the test data.\n",
        "# Ensure the model is not overfitting (very high performance on training, poor on testing).\n",
        "\n",
        "# 9. Model Deployment\n",
        "\n",
        "# Once the model has been evaluated and validated, it's ready for deployment. You can integrate it into a production environment or system for making predictions on new data.\n",
        "\n",
        "# 10. Monitor and Maintain the Model\n",
        "\n",
        "# Monitor the model's performance over time. As new data becomes available, you may need to retrain the model or update it.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x4ozMJ9_7OsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "\n",
        "\n",
        "# Exploratory Data Analysis (EDA) is a crucial step in the data science and machine learning process,\n",
        "# and it helps ensure that the model training is more effective, accurate, and efficient. Here are the key reasons why EDA is performed before fitting a model:\n",
        "\n",
        "# I. Understanding the Data\n",
        "\n",
        "# Insight into Features: EDA helps you understand the nature of the features (variables) in your dataset, whether they are categorical, numerical, or time-based.\n",
        "# Distribution of Features: You can visualize how the features are distributed (e.g., skewed, uniform, or normal distribution), which can inform decisions about transformations (e.g., scaling or normalization).\n",
        "# Correlations: By visualizing correlations between features, you can identify potential relationships or redundancies. This helps in feature selection and deciding which features to keep or drop.\n",
        "\n",
        "# Example: If a feature has little or no variance (e.g., most values are the same), it may not provide useful information for the model.\n",
        "\n",
        "\n",
        "# II. Handling Missing Data\n",
        "\n",
        "# Missing Values: During EDA, you will identify if any features have missing data, which is common in real-world datasets. Handling missing data is essential to prevent errors during model training.\n",
        "# we may decide to impute missing values (e.g., with the mean or median) or remove rows or columns with too many missing values.\n",
        "# Imputation techniques, such as mean imputation for numerical features or mode imputation for categorical features, can be considered.\n",
        "\n",
        "# Example: If a dataset has many missing values for a particular feature, it may indicate a need to rethink how that feature is handled.\n",
        "\n",
        "\n",
        "# III. Identifying Outliers\n",
        "\n",
        "# Outlier Detection: EDA helps you detect outliers (data points that differ significantly from others), which can negatively impact the performance of some models (e.g., linear regression, k-means clustering).\n",
        "# Handling Outliers: Outliers can be removed, transformed, or treated with robust algorithms that handle them more effectively.\n",
        "\n",
        "# Example: If you're working with a regression model and one data point is much larger than the rest, it may pull the regression line toward it, leading to a less accurate model.\n",
        "\n",
        "\n",
        "# IV. Feature Engineering\n",
        "\n",
        "# New Features: Based on domain knowledge and patterns identified during EDA, you can create new features that might improve the model’s predictive power.\n",
        "# Feature Transformation: EDA helps you decide whether certain features need to be transformed, such as scaling numerical values, encoding categorical variables, or applying logarithmic transformations to highly skewed data.\n",
        "\n",
        "# Example: If we find that one of our features is highly skewed, we might decide to log-transform it to normalize the distribution before fitting the model.\n",
        "\n",
        "\n",
        "# V. Detecting Data Quality Issues\n",
        "\n",
        "# Inconsistencies: EDA helps identify inconsistencies or errors in the dataset (e.g., duplicate entries, incorrect data types, unexpected categorical values).\n",
        "# Data Cleaning: By performing EDA, you can clean the data (e.g., remove duplicate rows, correct incorrect labels, etc.) before fitting the model.\n",
        "\n",
        "# Example: If a dataset contains categorical values like \"Yes\", \"No\", and \"yes\", we can standardize the values during EDA (to \"Yes\" and \"No\").\n",
        "\n",
        "\n",
        "# VI. Choosing the Right Model\n",
        "\n",
        "# Choosing the Right Model Type: EDA provides insight into whether the problem at hand is classification or regression, which in turn helps you choose the appropriate model.\n",
        "# For example, if the target variable is categorical, you may need to use a classification model (e.g., Logistic Regression, Random Forest).\n",
        "# If the target is continuous, you may need a regression model (e.g., Linear Regression, Decision Trees).\n",
        "# Feature Selection: You may identify features that are highly correlated or irrelevant, and decide to exclude them or transform them before feeding them into the model.\n",
        "\n",
        "# Example: If we have a categorical target variable, performing EDA helps you confirm that a classification model is the right choice.\n",
        "\n",
        "\n",
        "# VII.  Data Visualization\n",
        "\n",
        "# Visualizing Relationships: EDA involves plotting graphs (e.g., histograms, scatter plots, box plots) that provide valuable insights into relationships between features, their distribution, and the target variable.\n",
        "# Outlier and Pattern Detection: Plots make it easier to spot unusual patterns, correlations, or outliers, which could be important in understanding how the model might behave with the data.\n",
        "\n",
        "# Example: A scatter plot between two variables might show a clear linear relationship, which suggests that a linear model could be appropriate.\n",
        "\n",
        "\n",
        "# VIII. Assessing Assumptions\n",
        "\n",
        "# Model Assumptions: Many machine learning models have assumptions about the data (e.g., linearity for linear regression, independence of features in Naive Bayes).\n",
        "# Normality of Features: Some models assume that features are normally distributed, and EDA can help you assess whether this assumption holds, enabling you to make informed decisions about transformations or model choice.\n",
        "\n",
        "# Example: In linear regression, we can assess the residuals to check if they are normally distributed, which is an assumption of the model.\n",
        "\n",
        "\n",
        "# IX. Improving Model Performance\n",
        "\n",
        "# Outlier Treatment and Feature Scaling: Handling outliers and scaling features appropriately during EDA can prevent performance degradation and help the model converge faster during training.\n",
        "# Class Imbalance: In classification problems, EDA helps you check for class imbalance (e.g., a dataset where 90% of the samples belong to one class). If class imbalance exists, you can choose appropriate techniques (like SMOTE or class weighting) during model training.\n",
        "\n",
        "# Example: In a binary classification problem with imbalanced classes, EDA can inform you to apply techniques such as oversampling the minority class or undersampling the majority class.\n",
        "\n",
        "\n",
        "# X. Providing a Roadmap for Model Fitting\n",
        "\n",
        "# EDA provides the groundwork for how the data should be preprocessed, what type of model to use, and how to handle specific data challenges.\n",
        "# By understanding the data first, many common mistakes can be avoided (e.g., applying a model to improperly scaled data or ignoring missing values), which can lead to poor model performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "db2AR2TvX-Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. What is correlation?\n",
        "\n",
        "\n",
        "# Correlation refers to the statistical relationship between two or more variables, showing whether and how strongly the variables are related to each other. In other words, it measures the degree to which one variable changes in response to changes in another variable.\n",
        "\n",
        "# Correlation can be:\n",
        "\n",
        "# Positive: When two variables move in the same direction. As one variable increases, the other also increases (or decreases together).\n",
        "# Negative: When two variables move in opposite directions. As one variable increases, the other decreases, and vice versa.\n",
        "# Zero or No Correlation: When there is no discernible relationship between the variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "oEEO-08LcPAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. What does negative correlation mean?\n",
        "\n",
        "\n",
        "# Negative correlation refers to a relationship between two variables where, as one variable increases, the other tends to decrease, and vice versa. In other words, the variables move in opposite directions. The stronger the negative correlation, the more predictable the change in one variable when the other changes.\n",
        "\n",
        "# Characteristics of Negative Correlation:\n",
        "\n",
        "# Inverse Relationship: A negative correlation means that an increase in one variable is associated with a decrease in the other variable.\n",
        "# Value Range: A correlation coefficient (denoted as r) for a negative correlation ranges from 0 to -1.\n",
        "# r = -1: Perfect negative correlation, meaning that as one variable increases, the other decreases in a perfectly predictable way.\n",
        "# r = -0.5: A moderate negative correlation, meaning the variables are somewhat inversely related, but not in a perfectly linear way.\n",
        "# r = 0: No correlation, meaning there is no apparent relationship between the two variables.\n",
        "\n",
        "# Example of Negative Correlation\n",
        "\n",
        "# Height and Distance to Ground:\n",
        "# As a person’s height increases, the distance between their feet and the ground decreases (for instance, when they're standing on the ground, the height of the feet is reduced). This could represent a negative correlation.\n",
        "\n",
        "# Temperature and Heating Costs:\n",
        "# As outdoor temperature increases (summer), the amount of energy spent on heating a house decreases. The relationship between temperature and heating costs is negative because higher temperatures are associated with lower heating costs.\n",
        "\n",
        "# Interpreting Negative Correlation:\n",
        "\n",
        "# Strong Negative Correlation (r close to -1):\n",
        "# This indicates that the two variables are almost perfectly inversely related. As one increases, the other decreases in a highly predictable manner.\n",
        "\n",
        "# Moderate Negative Correlation (r around -0.5):\n",
        "# There’s a noticeable inverse relationship, but it's not perfect. The variables are related, but there’s some variability.\n",
        "\n",
        "# Weak Negative Correlation (r closer to 0):\n",
        "# The variables still move in opposite directions, but the relationship is weak and not consistently predictable.\n",
        "\n",
        "\n",
        "# Importance of Negative Correlation:\n",
        "\n",
        "# Modeling Relationships:\n",
        "# Understanding negative correlation is useful when you are building models that rely on the interactions between variables.\n",
        "# For example, if you're predicting heating costs, you might use temperature as a predictor with a negative relationship.\n",
        "\n",
        "# Feature Selection:\n",
        "# If you have two features in your dataset that are negatively correlated with each other,\n",
        "# you may choose to keep one to avoid redundancy.\n",
        "# Highly correlated features (whether positive or negative) can lead to multicollinearity in models like linear regression, which may distort model estimates.\n",
        "\n",
        "# Investment and Economics:\n",
        "# In finance, negative correlations are important for diversification. For example,\n",
        "# the price of oil and the stock market sometimes have a negative correlation: when one goes up, the other goes down, which can help manage risk in investment portfolios.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B-Kgx5gQdIZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. How can you find correlation between variables in Python?\n",
        "\n",
        "\n",
        "# To find the correlation between variables in Python, you can use several methods;\n",
        "\n",
        "# 1. Using pandas for Correlation:\n",
        "\n",
        "# pandas provides the DataFrame.corr() method to compute pairwise correlation of columns in a DataFrame.\n",
        "# Example:\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create a sample DataFrame\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(\"Correlation_matrix_1 = \", correlation_matrix)\n",
        "\n",
        "\n",
        "# 2. Using numpy for Pearson Correlation\n",
        "\n",
        "# numpy has a corrcoef() function that can calculate the Pearson correlation coefficient between two arrays.\n",
        "# Example:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create sample arrays\n",
        "x = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([5, 4, 3, 2, 1])\n",
        "\n",
        "# Compute Pearson correlation coefficient\n",
        "correlation = np.corrcoef(x, y)[0, 1]\n",
        "\n",
        "print(\"Correlation_matrix_2 = \", correlation)\n",
        "\n",
        "\n",
        "# 3. Using scipy for Pearson and Spearman Correlation:\n",
        "\n",
        "# scipy.stats offers multiple correlation methods like Pearson, Spearman, and Kendall's Tau.\n",
        "# Pearson: Measures linear correlation.\n",
        "# Spearman: Measures rank-based correlation (non-parametric).\n",
        "# Kendall: Measures ordinal association between variables.\n",
        "# Example:\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "\n",
        "# Sample data\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y = [5, 4, 3, 2, 1]\n",
        "\n",
        "# Pearson correlation\n",
        "pearson_corr, _ = pearsonr(x, y)\n",
        "print(f\"Pearson correlation: {pearson_corr}\")\n",
        "\n",
        "# Spearman correlation\n",
        "spearman_corr, _ = spearmanr(x, y)\n",
        "print(f\"Spearman correlation: {spearman_corr}\")\n",
        "\n",
        "# Kendall correlation\n",
        "kendall_corr, _ = kendalltau(x, y)\n",
        "print(f\"Kendall correlation: {kendall_corr}\")\n",
        "\n",
        "\n",
        "# 4. Visualizing Correlation\n",
        "\n",
        "# You can also visualize the correlation using a heatmap, which is a common approach.\n",
        "# Example:\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Correlation matrix\n",
        "corr = df.corr()\n",
        "\n",
        "# Heatmap\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt='.2f')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 5. Interpreting Correlation\n",
        "\n",
        "# I. Pearson Correlation: Measures linear relationships. A correlation of 1 means perfect positive linear correlation, -1 means perfect negative correlation, and 0 means no linear correlation.\n",
        "# II. Spearman/Kendall: These can capture monotonic relationships (not necessarily linear).\n",
        "\n",
        "# These methods help in identifying the strength and direction of relationships between variables.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9X_A6T87g7tM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "518a69d4-fa64-4f6d-d06e-d44e5fd53de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation_matrix_1 =       A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n",
            "Correlation_matrix_2 =  -0.9999999999999999\n",
            "Pearson correlation: -1.0\n",
            "Spearman correlation: -0.9999999999999999\n",
            "Kendall correlation: -0.9999999999999999\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "\n",
        "# causation\n",
        "\n",
        "# Causation refers to a relationship between two variables where one variable directly causes the change in the other.\n",
        "# In a causal relationship, a change in the independent variable (cause) results in a change in the dependent variable (effect). Causation implies that the cause is responsible for the effect and there is a direct cause-and-effect relationship.\n",
        "\n",
        "# For example, if you heat water to 100°C, it will boil. In this case, heating the water causes it to boil. There is a direct cause-effect relationship, and one variable (temperature) leads to a change in the other (water boiling).\n",
        "\n",
        "# Difference Between Correlation and Causation:\n",
        "\n",
        "# 1. Correlation:\n",
        "# Definition: Correlation refers to a statistical relationship between two variables, where they tend to change together. However, correlation does not imply that one variable causes the other to change. The variables could be linked due to other factors or coincidences.\n",
        "# Example: There might be a strong correlation between the number of ice cream sales and the number of drowning incidents. This does not mean that buying ice cream causes drowning.\n",
        "\n",
        "# 2. Causation:\n",
        "# Definition: Causation is when one variable directly affects the other. This is a more specific relationship than correlation and requires evidence of a direct cause-and-effect link, often determined through controlled experiments or statistical methods that account for other influencing factors.\n",
        "# Example: If you increase the amount of fertilizer applied to plants, the plants will likely grow better. In this case, fertilizer causes plant growth to improve.\n",
        "\n",
        "# Key Differences:\n",
        "# Correlation can be due to chance, external factors, or hidden confounders. It only measures the strength and direction of a relationship between variables.\n",
        "# Causation requires evidence that a change in one variable directly causes a change in the other, often supported by experimental data or logical reasoning.\n",
        "\n",
        "# Example to Illustrate the Difference:\n",
        "\n",
        "# Correlation Example:\n",
        "# Let's say we observe the following:\n",
        "\n",
        "# Increased sales of ice cream in the summer\n",
        "# Increased number of drowning incidents in the summer\n",
        "# While these two events correlate (both increase in summer), we cannot conclude that eating ice cream causes drowning.\n",
        "# In reality, the seasonal heat is the cause of both — more people swim in summer, which can lead to more drowning incidents and more people eat ice cream to cool off.\n",
        "\n",
        "# Causation Example:\n",
        "# Now, consider a study where:\n",
        "\n",
        "# Consuming a higher dose of a specific drug (independent variable) leads to a reduction in pain (dependent variable)\n",
        "# Here, we have evidence of causation because the study was controlled to isolate the drug's effect, ensuring no other variable influenced the outcome. In this case, the drug directly causes pain relief.\n",
        "\n",
        "# The Common Phrase: \"Correlation Does Not Imply Causation\"\n",
        "# This phrase is often used to causation against drawing incorrect conclusions from correlated data. Just because two variables appear to be related does not mean that one is causing the other. There could be an underlying factor at play or the relationship could be purely coincidental.\n",
        "\n",
        "# Conclusion:\n",
        "# Correlation shows that two variables are related, but it doesn't prove that one causes the other.\n",
        "# Causation demonstrates that one variable directly causes the change in another variable, and it requires strong evidence, often from experiments or robust statistical analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pv68RFiGIk4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "\n",
        "\n",
        "In the context of machine learning and deep learning, an optimizer is an algorithm used to adjust the parameters (such as weights and biases in neural networks) of a model during training to minimize the loss function or cost function. The goal of an optimizer is to find the set of parameters that result in the best performance of the model (i.e., minimize the error or improve accuracy).\n",
        "\n",
        "The process of optimization typically involves:\n",
        "\n",
        "Calculating the gradient of the loss function with respect to the model's parameters (using techniques like backpropagation).\n",
        "Updating the parameters in the direction that reduces the loss function (typically using gradient descent or variants).\n",
        "Types of Optimizers\n",
        "There are several types of optimizers, each with its own approach to adjusting the model's parameters. The most common types include:\n",
        "\n",
        "###1. Gradient Descent (GD)\n",
        "Gradient Descent is the most basic optimizer and works by updating the parameters in the direction of the negative gradient of the loss function with respect to the parameters.\n",
        "\n",
        "Formula:\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "\n",
        "Where:\n",
        "θ are the model parameters.\n",
        "\n",
        "𝜂 is the learning rate (step size).\n",
        "\n",
        "∇\n",
        "θ\n",
        "​\n",
        " J(θ) is the gradient of the loss function\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "J(θ) with respect to\n",
        "𝜃\n",
        "θ.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "In a simple linear regression, you use gradient descent to minimize the mean squared error (MSE) between predicted values and actual data points. The parameters (weights) are updated iteratively by moving in the direction opposite to the gradient to reduce the MSE.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Simple and easy to implement.\n",
        "Suitable for convex optimization problems.\n",
        "Cons:\n",
        "\n",
        "May converge slowly.\n",
        "Sensitive to the learning rate.\n",
        "\n",
        "##2. Stochastic Gradient Descent (SGD)\n",
        "Stochastic Gradient Descent is a variant of gradient descent that uses only one training example (instead of the entire dataset) to compute the gradient and update the parameters. It is more computationally efficient but introduces more noise, leading to a more erratic path toward the minimum.\n",
        "\n",
        "Formula:\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "⋅\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ",\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "\n",
        "Where\n",
        "𝑥\n",
        "𝑖\n",
        "  and\n",
        "𝑦\n",
        "𝑖\n",
        "are individual training data points.\n",
        "\n",
        "Example:\n",
        "\n",
        "In training a neural network, instead of using the full dataset to compute the gradient, you randomly pick one example and update the weights accordingly, repeating this process for multiple iterations.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Faster than batch gradient descent since it updates the weights more frequently.\n",
        "Helps escape local minima due to noise.\n",
        "\n",
        "Cons:\n",
        "\n",
        "The optimization path is noisier, and the process can be unstable.\n",
        "It may require more epochs to converge.\n",
        "\n",
        "##3. Mini-Batch Gradient Descent\n",
        "Mini-Batch Gradient Descent combines both batch gradient descent and stochastic gradient descent. It splits the dataset into small batches (instead of using one data point or the entire dataset) and updates the model's parameters using these mini-batches.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Same as gradient descent but applied to a small batch of data.\n",
        "Example:\n",
        "\n",
        "If you have 1000 training samples, you can divide them into mini-batches of size 32. For each mini-batch, you compute the gradient and update the weights. This combines the computational efficiency of batch gradient descent and the speed of SGD.\n",
        "\n",
        "Pros:\n",
        "\n",
        "More efficient than full batch gradient descent.\n",
        "Offers a balance between stability and noise.\n",
        "Can take advantage of parallelization.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Requires tuning the batch size.\n",
        "Can still be noisy compared to full batch gradient descent.\n",
        "\n",
        "##4. Momentum\n",
        "The Momentum optimizer is an extension of gradient descent that helps accelerate convergence by adding a fraction of the previous update to the current update. This prevents oscillations and smoothens the path toward the minimum.\n",
        "\n",
        "Formula:\n",
        "𝑣\n",
        "𝑡\n",
        "=\n",
        "𝛽\n",
        "𝑣\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛽\n",
        ")\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "\n",
        "θ=θ−η⋅v\n",
        "t\n",
        "​\n",
        "\n",
        "\n",
        "Where:\n",
        "𝑣\n",
        "𝑡\n",
        "​\n",
        "  is the velocity (update).\n",
        "\n",
        "𝛽 is the momentum factor (typically close to 0.9).\n",
        "\n",
        "Example:\n",
        "\n",
        "In training a deep neural network, momentum helps the optimizer keep moving in directions that have had consistent gradients in the past, reducing the oscillations and speeding up convergence.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Helps escape local minima.\n",
        "Speeds up convergence, especially in regions with shallow gradients.\n",
        "Cons:\n",
        "\n",
        "Requires tuning of momentum parameter.\n",
        "Can overshoot the optimum if the momentum term is too large.\n",
        "##5. RMSprop (Root Mean Square Propagation)\n",
        "RMSprop is an adaptive learning rate method that adjusts the learning rate for each parameter individually. It divides the learning rate by a moving average of recent squared gradients for each parameter.\n",
        "\n",
        "Formula:\n",
        "𝑣\n",
        "𝑡\n",
        "=\n",
        "𝛽\n",
        "𝑣\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛽\n",
        ")\n",
        "(\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        ")\n",
        "2\n",
        "\n",
        "\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "𝑣\n",
        "𝑡\n",
        "+\n",
        "𝜖\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "\n",
        "Where:\n",
        "𝑣\n",
        "𝑡\n",
        "​\n",
        "  is the moving average of squared gradients.\n",
        "\n",
        "𝜖 is a small constant to prevent division by zero.\n",
        "\n",
        "Example:\n",
        "\n",
        "RMSprop is often used when training recurrent neural networks (RNNs) or any deep learning models where learning rate adjustments are needed to avoid instability in training.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Efficient for problems with non-stationary objectives (e.g., RNNs).\n",
        "Helps adapt to different gradient magnitudes.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Requires tuning of the learning rate and decay factor.\n",
        "May be less effective for convex problems compared to other methods.\n",
        "##6. Adam (Adaptive Moment Estimation)\n",
        "Adam is one of the most popular optimizers, combining the benefits of Momentum and RMSprop. It computes adaptive learning rates for each parameter by combining both first-order momentum (average of gradients) and second-order momentum (average of squared gradients).\n",
        "\n",
        "Formula:\n",
        "\n",
        "mt​=β1​mt−1​+(1−β1​)∇θ​J(θ)\n",
        "\n",
        "vt​\\=β2​vt−1​+(1−β2​)(∇θ​J(θ))2\n",
        "\n",
        "mt​^​\\=1−β1t​mt​​,vt​^​\\=1−β2t​vt​​\n",
        "\n",
        "θ\\=θ−vt​^​​+ϵη​mt​^\n",
        "\n",
        "Example:\n",
        "\n",
        "Adam is widely used for training large-scale deep learning models. For example, when training a convolutional neural network (CNN) for image classification, Adam adjusts the learning rate for each parameter and accelerates convergence.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Works well for large datasets and high-dimensional parameter spaces.\n",
        "Requires less tuning than momentum or SGD.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Can sometimes lead to overfitting due to its powerful adaptive nature.\n",
        "Can be computationally expensive for small datasets.\n",
        "##7. Adagrad\n",
        "Adagrad is another adaptive learning rate method that adjusts the learning rate based on the historical gradient. It adapts more quickly for infrequent updates and less quickly for frequent updates.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝐺\n",
        "𝑡\n",
        "=\n",
        "𝐺\n",
        "𝑡\n",
        "−\n",
        "1\n",
        "+\n",
        "(\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        ")\n",
        "2\n",
        "\n",
        "\n",
        "𝜃\n",
        "=\n",
        "𝜃\n",
        "−\n",
        "𝜂\n",
        "𝐺\n",
        "𝑡\n",
        "+\n",
        "𝜖\n",
        "∇\n",
        "𝜃\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "\n",
        "Example:\n",
        "\n",
        "In sparse data scenarios (e.g., natural language processing with large vocabularies), Adagrad can be effective by adjusting learning rates for infrequent features.\n",
        "\n",
        "Pros:\n",
        "\n",
        "Suitable for sparse data.\n",
        "Automatically adjusts the learning rate.\n",
        "\n",
        "Cons:\n",
        "\n",
        "The learning rate can become too small, leading to premature convergence.\n",
        "It can be unstable for certain types of problems."
      ],
      "metadata": {
        "id": "AR2h34arguEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. What is sklearn.linear_model ?\n",
        "\n",
        "\n",
        "# sklearn.linear_model is a module in the scikit-learn library, which provides a collection of algorithms for linear models used in machine learning. These models are typically used for regression and classification tasks, where the relationship between the dependent (target) variable and one or more independent (input) variables is assumed to be linear.\n",
        "\n",
        "# Key Concepts in sklearn.linear_model\n",
        "# Linear models make predictions based on a weighted sum of the input features. In the case of regression, the goal is to find a line (or hyperplane in higher dimensions) that best fits the data. In classification, the goal is to separate classes with a hyperplane.\n",
        "\n",
        "# Key Classes in sklearn.linear_model:\n",
        "\n",
        "# Linear Regression (LinearRegression)\n",
        "# Used for predicting continuous values based on linear relationships between input features and target.\n",
        "\n",
        "# Use case: Predicting house prices based on features like square footage, number of rooms, etc.\n",
        "\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a simple regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "\n",
        "# 2. Logistic Regression (LogisticRegression)\n",
        "\n",
        "# A classification algorithm used for binary or multiclass classification problems. Despite its name, it's used for classification, not regression.\n",
        "\n",
        "# Use case: Predicting whether an email is spam or not (binary classification).\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Create and train the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(predictions)\n",
        "\n",
        "# 3. Ridge Regression (Ridge)\n",
        "\n",
        "# A variant of linear regression that includes a regularization term to penalize large coefficients. This helps to prevent overfitting by discouraging large values for the model parameters.\n",
        "\n",
        "# Use case: Used when there's multicollinearity (correlation between features) or to prevent overfitting in regression models.\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Create and train a ridge regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "ridge_predictions = ridge_model.predict(X_test)\n",
        "\n",
        "print(ridge_predictions)\n",
        "\n",
        "\n",
        "# 4. Lasso Regression (Lasso)\n",
        "\n",
        "# Similar to Ridge Regression but with L1 regularization, which encourages sparsity (driving some coefficients to zero). This can be used for feature selection as well as regression.\n",
        "\n",
        "# Use case: When you need to both regularize the model and reduce the number of features used.\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Create and train a lasso regression model\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "lasso_predictions = lasso_model.predict(X_test)\n",
        "\n",
        "print(lasso_predictions)\n",
        "\n",
        "\n",
        "# 5. ElasticNet\n",
        "\n",
        "# A linear regression model that combines both Lasso and Ridge regression (L1 + L2 regularization).\n",
        "# It balances the penalties of both L1 and L2 regularization to encourage sparsity and avoid overfitting.\n",
        "\n",
        "# Use case: When you want a more flexible regularization method that can perform well in a variety of scenarios.\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "\n",
        "# Create and train an ElasticNet model\n",
        "elastic_model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # l1_ratio balances L1 and L2\n",
        "elastic_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "elastic_predictions = elastic_model.predict(X_test)\n",
        "\n",
        "print(elastic_predictions)\n",
        "\n",
        "\n",
        "# 6. Poisson Regression (PoissonRegressor)\n",
        "\n",
        "# A regression model for count data that assumes the target variable follows a Poisson distribution.\n",
        "# It is often used in cases where the target variable represents counts or rates (e.g., number of events in a fixed interval of time).\n",
        "\n",
        "# Use case: Modeling count data such as the number of customers arriving at a store per day.\n",
        "\n",
        "# Example:\n",
        "\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "\n",
        "# Create and train a Poisson regression model\n",
        "poisson_model = PoissonRegressor(alpha=1.0)\n",
        "poisson_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "poisson_predictions = poisson_model.predict(X_test)\n",
        "\n",
        "print(poisson_predictions)\n",
        "\n",
        "\n",
        "# These models are powerful and flexible tools for regression and classification tasks.\n",
        "# The choice of which to use depends on the problem, the type of data, and the regularization needs of your model.\n",
        "\n"
      ],
      "metadata": {
        "id": "KRjROWbjc-_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "\n",
        "# In scikit-learn, the model.fit() method is used to train a machine learning model on a given dataset.\n",
        "# This is the key step where the model learns the patterns or relationships from the training data.\n",
        "\n",
        "# Training the Model: The fit() method takes the training data and adjusts the model's internal parameters (such as coefficients, weights, etc.) to minimize the error or loss function.\n",
        "# Learning Process: In supervised learning, the model will use the input features (independent variables) and the target values (dependent variable) to learn the relationship. For unsupervised learning, the model will try to identify patterns or groupings in the data without any predefined target.\n",
        "\n",
        "# General Syntax: model.fit(X, y)\n",
        "\n",
        "# Where:\n",
        "\n",
        "# X is the input data (features) in the form of a 2D array or DataFrame, with each row representing a sample and each column representing a feature.\n",
        "# y is the target data (labels or output) in the form of a 1D array or vector. It contains the corresponding labels (for classification) or continuous values (for regression).\n",
        "\n",
        "# Arguments to fit()\n",
        "\n",
        "# X (required): The input data (features).\n",
        "# Type: 2D array-like (such as numpy.ndarray, pandas.DataFrame, etc.)\n",
        "# Shape: (n_samples, n_features), where n_samples is the number of samples (data points) and n_features is the number of features (variables) in the dataset.\n",
        "\n",
        "# y (required for supervised learning): The target values (labels).\n",
        "# Type: 1D array-like (such as numpy.ndarray, pandas.Series, etc.)\n",
        "# Shape: (n_samples,), where n_samples is the number of samples.\n",
        "# For unsupervised learning (e.g., clustering), y is often not required, since the model learns from the input data without needing target values.\n",
        "\n",
        "# Special Cases:\n",
        "# Unsupervised Learning: For algorithms like clustering (e.g., KMeans), the fit() method may only require X and not y, because there are no target labels to predict.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WFLPPBJBwoLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "\n",
        "# The model.predict() method in scikit-learn is used to make predictions using a trained machine learning model.\n",
        "# Once you have trained a model using the fit() method, you can use predict() to apply the model to new, unseen data and obtain predictions.\n",
        "\n",
        "# Prediction: The predict() method uses the learned parameters (e.g., weights, coefficients, cluster centers) from the training process to make predictions on new input data.\n",
        "# Output:\n",
        "\n",
        "# For supervised learning tasks:\n",
        "# In regression, it predicts continuous numerical values.\n",
        "# In classification, it predicts the class labels (or categories) for each sample.\n",
        "\n",
        "# For unsupervised learning models (e.g., clustering),\n",
        "# predict() may return cluster assignments or other groupings, depending on the model type.\n",
        "\n",
        "# General Syntax: predictions = model.predict(X)\n",
        "\n",
        "# Where:\n",
        "# X is the input data (features) for which we want to make predictions.\n",
        "# Type: 2D array-like (e.g., numpy.ndarray, pandas.DataFrame, etc.)\n",
        "# Shape: (n_samples, n_features), where n_samples is the number of new data points, and n_features is the number of features (variables).\n",
        "# predictions: The predicted values, which can be either class labels (for classification) or continuous values (for regression), depending on the model.\n",
        "\n",
        "# Arguments for model.predict()\n",
        "\n",
        "# X (required): The input features for which you want to make predictions.\n",
        "# Type: 2D array-like (such as numpy.ndarray, pandas.DataFrame, etc.)\n",
        "# Shape: (n_samples, n_features), where n_samples is the number of new data points and n_features is the number of features.\n",
        "\n",
        "# The input X should have the same number of features (n_features) as the data used to train the model (model.fit()).\n",
        "\n"
      ],
      "metadata": {
        "id": "6au99rZUzFjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "# In data analysis and machine learning, variables (also known as features or attributes) are typically classified into two broad categories based on their nature: continuous variables and categorical variables.\n",
        "\n",
        "# 1. Continuous Variables:\n",
        "\n",
        "# Continuous variables are numeric variables that can take any value within a given range.\n",
        "# These variables can have an infinite number of values and can be measured with great precision.\n",
        "# Continuous variables are often associated with measurements or quantities.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Numerical: They are represented by numbers and can be used in mathematical operations like addition, subtraction, multiplication, and division.\n",
        "# Infinite Possibilities: Continuous variables can take any value within a range (e.g., temperature, height, weight, etc.).\n",
        "# Can be Subdivided: These variables can have decimal points (e.g., 3.14, 17.58).\n",
        "\n",
        "# Examples:\n",
        "# Temperature: 20°C, 20.1°C, 20.01°C, etc.\n",
        "# Height: 5.7 feet, 6.02 feet, etc.\n",
        "# Weight: 70.5 kg, 75.2 kg, etc.\n",
        "# Time: 12.25 seconds, 2.4 hours, etc.\n",
        "# Income: 50000.0, 100000.0, etc.\n",
        "\n",
        "# Use Case:\n",
        "# Continuous variables are often used in regression models because the output is a continuous value. For example, predicting house prices or stock market trends involves continuous variables (e.g., price, volume).\n",
        "\n",
        "# 2. Categorical Variables:\n",
        "\n",
        "# Categorical variables represent categories or distinct groups that cannot be ordered in a meaningful numerical manner.\n",
        "# These variables can take a limited number of values that represent different categories or labels.\n",
        "\n",
        "# Characteristics:\n",
        "\n",
        "# Non-Numeric: The values of categorical variables are typically not numbers. They can be in the form of labels or names.\n",
        "# Discrete Values: The values are distinct and limited. There is no concept of ordering or measuring between categories, except in cases where there is an inherent order (this is known as ordinal variables).\n",
        "\n",
        "# Examples:\n",
        "# Gender: Male, Female\n",
        "# Color: Red, Blue, Green\n",
        "# Marital Status: Single, Married, Divorced\n",
        "# Country: USA, India, France, etc.\n",
        "# Types of Categorical Variables:\n",
        "\n",
        "# Nominal Variables: These are categorical variables with no intrinsic ordering. For example, Color (Red, Blue, Green) or Country (USA, India, France) are nominal because there is no natural order between these categories.\n",
        "\n",
        "# Ordinal Variables: These are categorical variables where the categories have a meaningful order, but the differences between categories are not necessarily uniform. For example, Education Level (High School, Bachelor's, Master's, Ph.D.) has an inherent order, but the \"distance\" between levels is not the same.\n",
        "\n",
        "# Use Case:\n",
        "# Categorical variables are used in classification models, where the goal is to predict a class or category. For example, predicting whether an email is \"spam\" or \"not spam\" involves a categorical variable (spam/not spam).\n",
        "\n"
      ],
      "metadata": {
        "id": "hM1Yyat_35di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "       22. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. The goal is to transform the features so that they have similar scales or ranges. This helps machine learning algorithms to perform better and converge faster.\n",
        "\n",
        "##Importance of Feature Scaling :\n",
        "\n",
        "I. Different Scales Across Features:\n",
        "\n",
        "In real-world datasets, features often have very different scales.\n",
        "For example, one feature could be age ranging from 1 to 100, while another feature could be income ranging from 10,000 to 1,000,000.\n",
        "If the features are not scaled, the model may be biased toward features with larger ranges.\n",
        "\n",
        "II. Impact on Distance-Based Algorithms:\n",
        "\n",
        "Many machine learning algorithms, such as K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and K-Means clustering, rely on distance calculations.\n",
        "Features with larger scales will dominate the distance calculations, potentially making the model's predictions less accurate.\n",
        "\n",
        "III. Gradient-Based Optimization Algorithms:\n",
        "\n",
        "Algorithms that use gradient descent (e.g., Linear Regression, Logistic Regression, Neural Networks) benefit from scaling because it allows them to converge faster.\n",
        "Without feature scaling, the optimization process can be slower and less stable due to the differing magnitudes of the features.\n",
        "\n",
        "\n",
        "###Role of Feature Scaling in Machine Learning:\n",
        "\n",
        "I. Improves Performance:\n",
        "\n",
        "For algorithms that depend on calculating distances (e.g., KNN, SVM), scaling ensures that all features contribute equally to the distance computation, leading to more accurate models.\n",
        "\n",
        "II. Faster Convergence:\n",
        "\n",
        "For gradient-based optimization algorithms (like linear regression, neural networks), scaling leads to faster convergence during the training phase because the gradients will be more evenly distributed across the features.\n",
        "\n",
        "III. Prevents Model Bias:\n",
        "\n",
        "Feature scaling ensures that the machine learning model does not give more importance to a feature just because it has a larger range. This helps avoid bias towards certain features and improves model interpretability and accuracy.\n",
        "\n",
        "\n",
        "##Common Methods of Feature Scaling\n",
        "\n",
        "\n",
        "I. Min-Max Scaling (Normalization)\n",
        "\n",
        "Min-max scaling transforms features to a specific range, usually between 0 and 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "X\n",
        "scaled\n",
        "​\n",
        " =\n",
        "(X−min(X)) / (max(X)−min(X))\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "X is the original value of a feature,\n",
        "min\n",
        "⁡\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "min(X) is the minimum value of that feature,\n",
        "max\n",
        "⁡\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "max(X) is the maximum value of that feature.\n",
        "\n",
        "Use Case: Useful when the data is bounded or you want to preserve the relationships between features.\n",
        "\n",
        "II. Standardization (Z-score Scaling)\n",
        "\n",
        "Standardization (also known as Z-score normalization) transforms the features to have a mean of 0 and a standard deviation of 1. It is less sensitive to outliers than min-max scaling.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "scaled\n",
        "=\n",
        "(\n",
        "𝑋\n",
        "−\n",
        "𝜇\n",
        ")\n",
        "/\n",
        "𝜎\n",
        "\n",
        "where:\n",
        "\n",
        "\n",
        "X is the original value of a feature,\n",
        "𝜇\n",
        "μ is the mean of the feature,\n",
        "\n",
        "σ is the standard deviation of the feature.\n",
        "\n",
        "Use Case: Preferred when the data follows a normal distribution or for algorithms that assume normally distributed data (e.g., Linear Regression, Logistic Regression, PCA).\n",
        "\n",
        "III. Robust Scaling\n",
        "\n",
        "Robust scaling scales features using the median and interquartile range (IQR). This method is less sensitive to outliers, unlike standardization and min-max scaling.\n",
        "\n",
        "Formula:\n",
        "\n",
        "\n",
        "X\n",
        "scaled\n",
        "​\n",
        " =\n",
        "(X−Median(X)) / IQR(X)\n",
        "​\n",
        "\n",
        "where:\n",
        "\n",
        "Median\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "Median(X) is the median of the feature,\n",
        "IQR\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "IQR(X) is the interquartile range of the feature (difference between the 75th and 25th percentiles).\n",
        "\n",
        "Use Case: Useful when the dataset contains significant outliers that would affect other scaling methods.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pcQLZqsk9APN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    23. How do we perform scaling in Python?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Scaling in Python typically refers to the process of adjusting the range of features or data values so they can be used effectively in machine learning models. This is done to ensure that features with larger ranges don't dominate those with smaller ranges. Commonly, scaling is performed using libraries such as scikit-learn or by implementing custom functions.\n",
        "\n",
        "Here are a few common methods of scaling in Python:\n",
        "\n",
        "### 1. Standardization (Z-Score Scaling)\n",
        "\n",
        "Standardization transforms the data such that the mean becomes 0 and the standard deviation becomes 1. This is useful for algorithms that assume data is normally distributed (e.g., linear regression, logistic regression).\n",
        "\n",
        "Formula:\n",
        "\n",
        "Z= (X−μ) / σ\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "X is the original feature\n",
        "\n",
        "\n",
        "μ is the mean\n",
        "\n",
        "σ is the standard deviation\n",
        "\n",
        "\n",
        "\n",
        "### 2. Min-Max Scaling\n",
        "\n",
        "\n",
        "Min-Max scaling (or normalization) rescales the data into a fixed range, typically [0, 1]. This method is useful when the data needs to be within a specific range.\n",
        "\n",
        "Formula:\n",
        "\n",
        "\n",
        "X\n",
        "′\n",
        " =\n",
        " (X−X\n",
        "min) /\n",
        "(\n",
        "X\n",
        "max\n",
        "​\n",
        " −X\n",
        "min)\n",
        "​\n",
        "\n",
        "\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "X\n",
        "min\n",
        "​ is the minimum value in the feature\n",
        "\n",
        "X\n",
        "max\n",
        "​ is the maximum value in the feature\n",
        "\n",
        "\n",
        "### 3. MaxAbs Scaling\n",
        "\n",
        "MaxAbs scaling scales each feature by dividing by the maximum absolute value of that feature, such that the resulting values lie between -1 and 1.\n",
        "\n",
        "This is useful when the data is already centered around 0 and sparse data (with many zeros) is involved.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "′\n",
        "=\n",
        "𝑋 /\n",
        "∣\n",
        "𝑋\n",
        "max\n",
        "∣\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑋\n",
        "max\n",
        "​ is the maximum absolute value of the feature\n",
        "\n",
        "\n",
        "### 4. Robust Scaling\n",
        "\n",
        "\n",
        "Robust scaling is used to scale features using statistics that are robust to outliers, such as the median and interquartile range (IQR). This is helpful when the dataset contains significant outliers.\n",
        "\n",
        "Formula:\n",
        "\n",
        "𝑋\n",
        "′\n",
        "=\n",
        "(𝑋\n",
        "−\n",
        "Median) /\n",
        "IQR\n",
        "\n",
        "Where:\n",
        "\n",
        "Median is the middle value of the feature\n",
        "\n",
        "IQR is the interquartile range (Q3 - Q1)\n",
        "\n",
        "\n",
        "### 5. Log Transformation (for skewed data)\n",
        "\n",
        "Log scaling is used when the data is heavily skewed. By applying the logarithmic function, it can reduce the impact of large values and bring the data closer to a normal distribution.\n",
        "\n",
        "### 6. Custom Scaling\n",
        "If none of the built-in scaling methods meet your requirements, you can manually scale the features by applying custom functions or logic.\n",
        "\n"
      ],
      "metadata": {
        "id": "-iRCP2fi4OXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. What is sklearn.preprocessing?\n",
        "\n",
        "\n",
        "# sklearn.preprocessing is a module in the scikit-learn library in Python.\n",
        "# It provides a variety of utilities to pre-process your data before applying machine learning algorithms.\n",
        "# Preprocessing is an essential step in data preparation that involves transforming raw data into a format that is more suitable for modeling.\n",
        "\n",
        "# Here are some of the key functions and classes within sklearn.preprocessing:\n",
        "\n",
        "# StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "# MinMaxScaler: Scales and translates each feature individually such that it is in the given range, e.g., between zero and one.\n",
        "\n",
        "# Normalizer: Normalizes samples individually to unit norm.\n",
        "\n",
        "# Binarizer: Binarizes (sets feature values to 0 or 1) according to a threshold.\n",
        "\n",
        "# PolynomialFeatures: Generates a new feature matrix consisting of all polynomial combinations of the features with a specified degree.\n",
        "\n",
        "# LabelEncoder: Encodes labels with value between 0 and n_classes-1.\n",
        "\n",
        "# OneHotEncoder: Encodes categorical integer features as a one-hot numeric array.\n",
        "\n",
        "# Imputer: (Deprecated, use sklearn.impute.SimpleImputer instead) Imputes missing values.\n",
        "\n",
        "# Example of how to use MinMaxScaler:\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "print(scaled_data)\n",
        "\n",
        "# This will scale the data to fall within the range of 0 and 1.\n",
        "\n",
        "# Preprocessing helps improve the performance and accuracy of machine learning models.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwSmDOGe-wNM",
        "outputId": "aa7e5ccd-2807-4720-ede4-76b0e53d1e0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "\n",
        "# In Python, data splitting for model training and testing is commonly done using scikit-learn's train_test_split function.\n",
        "# This function randomly splits the dataset into two sets: one for training the model and one for testing it.\n",
        "# Typically, the data is split into a training set (used to fit the model) and a testing set (used to evaluate the model's performance).\n",
        "\n",
        "# 1. Using train_test_split from sklearn.model_selection\n",
        "\n",
        "# This is the most straightforward way to split your data into training and testing sets.\n",
        "\n",
        "# data: The data to be split (usually a NumPy array, pandas DataFrame, or similar).\n",
        "# test_size: Proportion of the data to be used for testing. For example, 0.2 means 20% of the data is used for testing and the remaining 80% is used for training. This value can also be an integer (the absolute number of test samples).\n",
        "# train_size: Alternatively, you can specify this to control the size of the training set.\n",
        "# random_state: A seed value for the random number generator to ensure reproducibility. Using the same value will give you the same split every time you run the code.\n",
        "# shuffle: Boolean (True by default). Determines if the data is shuffled before splitting.\n",
        "# stratify: This is used if you want the train and test sets to have the same distribution of a specific feature. For classification tasks, you can stratify by the target variable (i.e., labels).\n",
        "\n",
        "# 2. Example Usage\n",
        "\n",
        "# I. Basic Split Example (80% train, 20% test):\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n",
        "y = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Split data into 80% train and 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Testing Features:\", X_test)\n",
        "print(\"Training Labels:\", y_train)\n",
        "print(\"Testing Labels:\", y_test)\n",
        "\n",
        "# II. Stratified Split (Ensuring Equal Class Distribution in Train and Test Sets):\n",
        "\n",
        "# Stratified splitting is especially useful in classification tasks to ensure that both the training and test sets have a similar distribution of the target variable.\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n",
        "y = [0, 1, 0, 1, 0, 1]  # Binary class labels\n",
        "\n",
        "# Stratified split ensures the same proportion of each class in the train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Testing Features:\", X_test)\n",
        "print(\"Training Labels:\", y_train)\n",
        "print(\"Testing Labels:\", y_test)\n",
        "\n",
        "\n",
        "# 3. Advanced Split Options\n",
        "\n",
        "# I. Splitting Data into More than Two Sets (e.g., training, validation, and testing):\n",
        "\n",
        "# You can split your data into more than just two sets (train and test) by chaining train_test_split.\n",
        "# For example, if you need a validation set in addition to the training and testing sets:\n",
        "\n",
        "Example:\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset (features and labels)\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n",
        "y = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "# Split into 60% training, 20% validation, 20% test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Validation Features:\", X_val)\n",
        "print(\"Testing Features:\", X_test)\n",
        "\n",
        "# II. Using StratifiedKFold for Cross-Validation:\n",
        "\n",
        "# For cross-validation, StratifiedKFold ensures that each fold has the same proportion of target classes.\n",
        "# It is useful when you want to evaluate your model's performance over multiple folds of data.\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n",
        "y = [0, 1, 0, 1, 0, 1]\n",
        "\n",
        "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n",
        "    y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n",
        "    print(f\"Train: {X_train}, Test: {X_test}\")\n"
      ],
      "metadata": {
        "id": "g_JFhB00AjPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Explain data encoding?\n",
        "\n",
        "\n",
        "# Data encoding is the process of converting data into a different format.\n",
        "# This is crucial in machine learning and data preprocessing as it helps in converting categorical data into numerical form, which algorithms can understand and process efficiently.\n",
        "# Here are some common encoding techniques:\n",
        "\n",
        "# 1. Label Encoding\n",
        "\n",
        "# Label encoding converts categorical values into numerical values. Each unique category is assigned an integer value.\n",
        "# Example:\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample data\n",
        "data = ['red', 'green', 'blue', 'green', 'red', 'blue']\n",
        "\n",
        "# Initialize the encoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)\n",
        "\n",
        "# 2. One-Hot Encoding\n",
        "\n",
        "# One-hot encoding creates binary columns for each category. This avoids the issue of giving ordinal relationship to categorical data.\n",
        "# Example:\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = np.array(['red', 'green', 'blue', 'green', 'red', 'blue']).reshape(-1, 1)\n",
        "\n",
        "# Initialize the encoder\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(data).toarray()\n",
        "print(encoded_data)\n",
        "\n",
        "# 3. Binary Encoding\n",
        "\n",
        "# Binary encoding converts categories into binary numbers and then splits the digits into separate columns.\n",
        "\n",
        "# we can use the 'category_encoders' library for binary encoding\n",
        "!pip install category_encoders\n",
        "\n",
        "import category_encoders as ce\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({'color': ['red', 'green', 'blue', 'green', 'red', 'blue']})\n",
        "\n",
        "# Initialize the encoder\n",
        "encoder = ce.BinaryEncoder(cols=['color'])\n",
        "\n",
        "# Fit and transform the data\n",
        "encoded_data = encoder.fit_transform(data)\n",
        "print(encoded_data)\n",
        "\n",
        "# 4. Frequency Encoding\n",
        "\n",
        "# Frequency encoding replaces the categories by their respective frequency counts.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({'color': ['red', 'green', 'blue', 'green', 'red', 'blue']})\n",
        "\n",
        "# Calculate the frequency of each category\n",
        "frequency_encoding = data['color'].value_counts().to_dict()\n",
        "\n",
        "# Replace the categories with frequency counts\n",
        "data['color_encoded'] = data['color'].map(frequency_encoding)\n",
        "print(data)\n",
        "\n",
        "\n",
        "# These are just a few of the many encoding techniques available.\n",
        "# The choice of encoding depends on the specific problem and the nature of your data.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO0dK0goDirg",
        "outputId": "f7143a74-8a91-4335-b9dd-8eaca3295a6c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 1 0 1 2 0]\n",
            "[[0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]]\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.4-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.6.4-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.4\n",
            "   color_0  color_1\n",
            "0        0        1\n",
            "1        1        0\n",
            "2        1        1\n",
            "3        1        0\n",
            "4        0        1\n",
            "5        1        1\n",
            "   color  color_encoded\n",
            "0    red              2\n",
            "1  green              2\n",
            "2   blue              2\n",
            "3  green              2\n",
            "4    red              2\n",
            "5   blue              2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}